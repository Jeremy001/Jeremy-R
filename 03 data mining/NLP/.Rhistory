dt
set.seed(123)
df <- data.frame(a = rnorm(6),
b = runif(6, 1, 10),
c = letters[1:6])
df
dt <- data.table(a = rnorm(6),
b = runif(6, 1, 10),
c = letters[1:6])
dt
tables()
class(df)
class(dt)
dt[2,]
dt[dt$a < 0,]
dt[, 1]
dt[, 2:3]
dt[, c(2, 3)]
dt[, w:= a^2 * 10]
dt
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == 'swirl')
filter(cran, r_version == '3.1.1', country == 'US')
?Comparison
filter(cran, r_version <= '3.0.2', country == 'IN')
filter(cran, country == "US" | country == 'IN')
filter(cran, size > 100500, r_os == 'linux-gnu')
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(R.version))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_size = mean(size))
summarize(cran, avg_bytes = mean(size))
swirl()
install_from_swirl("Data_Analysis")
swirl()
swirl()
install_from_swirl("Exploratory_Data_Analysis")
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
swirl()
library(swirl)
swirl()
library(dplyr)
cran <- tal_df(mydf)
cran <- tbl_df(mydf)
rm(mydf)
rm("mydf")
cran
cran
group_by(cran,package)
?group_by()
help("group_by")
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
cran %>% select(ip_id, country, package, size) %>% print
submit()
submit()
submit()
submit()
submit()
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
remove.packages('jiebaR')
library(jiebaR)
library(jiebaR)
library(jiebaR)
wk = worker()
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
install_pkg <- function(pkg){
# 从列出的包中筛选出还未安装的包
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
# 如果有未安装的包，则安装该包和其依赖包
if(length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
# 加载列出的所有包
sapply(pkg, library, character.only = TRUE)
}
pkg <- c("ggplot2", "RJDBC", "recharts", "dplyr", "shiny", "shinydashboard", "DT",
"leaflet", "sqldf", "tidyr", "highcharter", "reshape2", "xts", "lubridate",
"jsonlite", "networkD3")
install_pkg(pkg)
## 创建JDBC连接
drv <-JDBC("oracle.jdbc.driver.OracleDriver",
"E:/R/ojdbc6.jar",
identifier.quote="`")
conn_b2btest_151 <-dbConnect(drv,
"jdbc:oracle:thin:@117.121.48.147:1521/bjdb01",
"b2btest",
"inet21")
cust_num <- dbGetQuery(
conn_b2btest_151,
"SELECT vc.sp_user_name,
sc.name,
COUNT(vc.cust_code) AS cust_num,
COUNT(CASE WHEN vc.create_date >= TO_DATE('2017-03-01', 'yyyy-mm-dd')
THEN vc.cust_code ELSE NULL END) AS cust_new
FROM JQQYNEW.vendor_cust@PRODDB vc
LEFT JOIN JQQYNEW.scuser@PRODDB sc ON vc.sp_user_name = sc.user_name
WHERE vendor_code = 'FYLYSM'
GROUP BY sp_user_name,
sc.name"
)
runApp('E:/0wlw/BI/shiny-bi/floatingfern')
data("mtcars")
head(mtcars)
library('dplyr')
data("iris")
head(iris)
head(iris)
str(iris)
iris2 <- tbl_df(iris)
iris2
iris2 <- tbl_df(iris)
iris2
str(iris2)
iris_s <- select(iris2, Species, Sepal.Length, Sepal.Width)
iris_s
iris_s <- select(iris2, Species, variable %in% c('Sepal.Width'))
library('dplyr')
library('dplyr')
iris2 <- tbl_df(iris)
iris2
str(iris2)
iris_s <- select(iris2, Species, variable %in% c('Sepal.Width'))
library(dplyr)
s2 <- select(iris, Species, contains('Sepal'))
head(s2)
library(dplyr)
library(dplyr)
data(iris)
head(iris)
library(dplyr)
data(iris)
head(iris)
iris2 <- select(iris, Sepal.Length, Sepal.Width, Species)
head(iris2)
iris3 <- select(iris, 1:2, 5)
head(iris3)
iris4 <- select(iris, contains('Sepal'))
head(iris4)
knitr::opts_chunk$set(echo = TRUE, include = TRUE)
str(airquality)
?select()
library(dplyr)
data(iris)
head(iris)
iris2 <- select(iris, Species)
head(iris2)
iris3 <- select(iris, 1:4)
head(iris3)
iris4 <- select(iris, Sepal.Length, Sepal.Width, Species)
head(iris4)
iris5 <- select(iris, contains('Sepal'))
head(iris5)
irir3 <- select(iris, -5)
head(iris3)
?select()
library(jiebaR)
wk = worker()
fruit <- 'apple orange grape banana'
nchar(fruit)
length(fruit)
x <- strsplit(fruit, split = ' ')
x
str(x)
fruitvec <- unlist(strsplit(fruit, split = ' '))
fruitvec
x <- paste(fruitvec, collapse = ',')
x
str(x)
x = paste0(fruitvec)
x
str(x)
x = paste0(fruitvec, collapse = ':')
x
str(x)
x = paste0(fruitvec, collapse = ';')
x
x <- substr(fruit, 1, 5)
x
fruit <- 'apple orange grape banana'
fruit
library(stringr)
strurl <- 'http://movie.douban.com/top250'
web <- readLines(strurl, encoding = 'UTF-8')
web[[1]]
str(web)
head(web)
Sys.getlocale('LC_ALL', 'Chinese')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
shiny::runApp('E:/0wlw/BI/shiny-bi/floatingfern')
install.packages("pbapply")
install.packages("pbapply")
install.packages(c("nycflights13", "gapminder", "Lahman"))
install.packages("plyr")
source('E:/R/Jeremy-R/03 data mining/NLP/jiebaR.r')
source('E:\R\Jeremy-R\03 data mining\NLP\jiebaR.r')
source('E:\\R\\Jeremy-R\\03 data mining\\NLP\\jiebaR.r')
library('rstudioapi')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(jiebaR)
wk = worker()
wk['大家好，我是来自杭州的Jeremy，一个数据分析爱好者']
wk['上海自来水来自海上']
wk['南京市长江大桥同志视察南京市长江大桥']
wk['南京市市长江大桥同志视察南京市长江大桥']
wk = worker(user = 'C:/Users/dakongyi/Documents/R/win-library/3.3/jiebaRD/dict/user.utf8')
wk['南京市长江大桥同志视察南京市长江大桥']
wk['南京市长江大桥同志视察南京市长江大桥']
library(devtools)
install_github('qinwf/cidian')
library(rvest)
url <- "https://www.psychologytoday.com/search/site/leadership%20competence"
n <- 2
urls <- paste(url, '?page=', 1:n, sep = '')
urls
urls <- c(url, urls)
urls
page <- read_html(urls[1])
href <- html_nodes(page, ".search-result h3 a")
href
href[1]
href <- html_nodes(page, ".search-result h3 a") %>%
html_attrs()
href
hrefs <- NULL
length(urls)
url <- "https://www.psychologytoday.com/search/site/leadership%20competencies"
n <- 2
urls <- paste(url, "?page=", 1:n, sep = "")
urls <- c(url, urls)
hrefs <- NULL
for i in 1:length(urls){
page <- read_html(urls[i])
href <- html_nodes(page, ".search-result h3 a") %>%
html_attrs()
hrefs <- c(hrefs, href)
}
for (i in 1:length(urls)){
page <- read_html(urls[i])
href <- html_nodes(page, ".search-result h3 a") %>%
html_attrs()
hrefs <- c(hrefs, href)
}
hrefs
hrefs <- NULL
for (i in 1:length(urls)) {
page <- read_html(urls[i])
href <- html_nodes(page, ".search-result h3 a") %>%
html_attrs()
hrefs <- c(hrefs, href)
}
hrefs
hrefs[[1]]
read_html(hrefs[[1]])
tmp_url <- read_html(hrefs[[1]])
text <- html_nodes(tmp_url, ".field-type-text-with-summary field__item")
text
text <- html_nodes(tmp_url, ".field-type-text-with-summary .field__item")
text
text <- html_nodes(tmp_url, ".field-type-text-with-summary .field__item") %>%
html_text()
text
texts <- NULL
for (i in 1:length(hrefs)){
tmp_url <- read_html(hrefs[[i]])
text <- html_nodes(tmp_url, ".field-type-text-with-summary .field__item") %>%
html_text()
texts <- c(texts, text)
}
texts
class(texts)
library(tm)
docs <- VCorpus(VectorSource(texts))
docs
class(texts)
inspect(docs[2])
inspect(docs[1:2])
meta(docs[2])
meta(docs[[2]])
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[2]]))
seq(docs)
docs[[i]] <- gsub("\n", " ", docs[[i]])
writeLines(as.character(docs[[2]]))
docs[[i]] <- gsub("\\n", " ", docs[[i]])
writeLines(as.character(docs[[2]]))
docs[[i]] <- gsub("\\\n", " ", docs[[i]])
writeLines(as.character(docs[[2]]))
docs
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, tolower)
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, removeWords, stopwords("en"))
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[[2]]))
dtm <- DocumentTermMatrix(docs)
dtm
tdm <- TermDocumentMatrix(docs)
tdm
inspect(dtm)
inspect(dtm[1:10, 1:5])
freq <- colSums(dtm)
freq <- colSums(as.matrix(dtm))
freq
sort(freq)
head(freq)
length(freq)
ord <- order(freq, decreasing = T)
freq(head(ord, 20))
freq[head(ord, 20)]
library(rvest)
library(tm)
url <- "https://www.psychologytoday.com/search/site/leadership%20competencies"
n <- 2
urls <- paste(url, "?page=", 1:n, sep = "")
urls <- c(url, urls)
## 前3个页面中所有文章(30篇)的url
hrefs <- NULL
for (i in 1:length(urls)) {
page <- read_html(urls[i])
href <- html_nodes(page, ".search-result h3 a") %>%
html_attrs()
hrefs <- c(hrefs, href)
}
## 30篇文章的所有内容
texts <- NULL
for (i in 1:length(hrefs)){
tmp_url <- read_html(hrefs[[i]])
text <- html_nodes(tmp_url, ".field-type-text-with-summary .field__item") %>%
html_text()
texts <- c(texts, text)
}
docs <- VCorpus(VectorSource(texts))
inspect(docs[1:2])
meta(docs[[2]])
writeLines(as.character(docs[[2]]))
# preprocessing
## 删除标点符号
docs <- tm_map(docs, removePunctuation)
## 删除换行符
for (i in seq(docs)) {
docs[[i]] <- gsub("\n", " ", docs[[i]])
}
## 删除所有数字
docs <- tm_map(docs, removeNumbers)
## 转换成小写
docs <- tm_map(docs, tolower)
## 删除所有停止词
docs <- tm_map(docs, removeWords, stopwords("en"))
## 还可以自定义停止词
docs <- tm_map(docs, removeWords, c("leadership", "leader", "leaders", "can", "ofen"))
# 英文单词有很多变体，比如时态，单数复数
# .......
# 最好的话，应该把不同变体都转换成相同的形式，作为一个单词
## 有些词可以组合成词组，以免被认作两个词
for (i in seq(docs)) {
docs[[i]] <- gsub("leadership competencies", "lead_comp", docs[[i]])
}
## 删除所有多于的空白
docs <- tm_map(docs, stripWhitespace)
## 最后整理一下
docs <- tm_map(docs, PlainTextDocument)
# 转换格式，查看文章中单词的频次
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
inspect(dtm[1:10, 1:5])
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq, decreasing = T)
freq[head(ord, 20)]
findAssocs(dtm, "leadership", "competencies", corlimit = 0.7)
findAssocs(dtm, c("leadership", "competencies"), corlimit = 0.7)
docs <- tm_map(docs, removePunctuation)
## 删除换行符
for (i in seq(docs)) {
docs[[i]] <- gsub("\n", " ", docs[[i]])
}
## 删除所有数字
docs <- tm_map(docs, removeNumbers)
## 转换成小写
docs <- tm_map(docs, tolower)
## 删除所有停止词
docs <- tm_map(docs, removeWords, stopwords("en"))
## 还可以自定义停止词
# docs <- tm_map(docs, removeWords, c("leadership", "leader", "leaders", "can", "ofen"))
# 英文单词有很多变体，比如时态，单数复数
# .......
# 最好的话，应该把不同变体都转换成相同的形式，作为一个单词
## 有些词可以组合成词组，以免被认作两个词
# for (i in seq(docs)) {
#   docs[[i]] <- gsub("leadership competencies", "lead_comp", docs[[i]])
# }
## 删除所有多于的空白
docs <- tm_map(docs, stripWhitespace)
## 最后整理一下
docs <- tm_map(docs, PlainTextDocument)
# 转换格式，查看文章中单词的频次
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
inspect(dtm[1:10, 1:5])
## 发现一个问题：由于前面没对单词的不同变体做处理，所以出现了同一单词的不同变体，比如ability
# 查看出现频次最高的20个单词
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq, decreasing = T)
freq[head(ord, 20)]
docs <- VCorpus(VectorSource(texts))
inspect(docs[1:2])
meta(docs[[2]])
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, removePunctuation)
for (i in seq(docs)) {
docs[[i]] <- gsub("\n", " ", docs[[i]])
}
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
inspect(dtm[1:10, 1:5])
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq, decreasing = T)
freq[head(ord, 20)]
findAssocs(dtm, c("leadership", "competencies"), corlimit = 0.7)
library(wordcloud2)
freq
class(freq)
?wordcloud2
library(tibble)
lead_comp <- as_tibble(freq)
lead_comp
lead_comp <- as_tibble(name(freq), freq)
lead_comp <- as_tibble(names(freq), freq)
names(freq)
lead_comp <- as_tibble(name = names(freq), freq)
lead_comp
lead_comp <- as_tibble(freq)
lead_comp
lead_comp$name <- names(freq)
lead_comp
word <- names(freq)
word
lead_comp <- as_tibble(word, freq)
wordcloud2(lead_comp, minSize = 25)
library(wordcloud)
wordcloud(names(freq), freq, min.freq = 25)
wordcloud2(lead_comp, minSize = 25, color = brewer.pal(6, "dark2"))
wordcloud2(lead_comp, minSize = 25, color = brewer.pal(6, "Dark2"))
wordcloud2(lead_comp, minSize = 25, colors = brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq = 25, colors = brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq = 25, colors = brewer.pal(10, "Dark2"))
